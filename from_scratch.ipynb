{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tgieruc/miniconda3/envs/noise2noise/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- BLOCKS -----\n",
    "class Module(object):\n",
    "    def __call__(self, input):\n",
    "        return self.forward(input)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        pass\n",
    "\n",
    "    def get_params(self):\n",
    "        return []\n",
    "\n",
    "    def set_params(self, params):\n",
    "        pass\n",
    "\n",
    "    def cuda(self):\n",
    "        pass\n",
    "\n",
    "    def params(self):\n",
    "        return []\n",
    "\n",
    "    def eval(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_fcts(gd, scratch):\n",
    "    x = torch.rand(5, 3, 32, 32, requires_grad=True)\n",
    "    y_gd = gd(x)\n",
    "    with torch.no_grad():\n",
    "        y_scratch = scratch(x)\n",
    "        y_gd.backward(torch.ones_like(y_gd))\n",
    "        y_grad_gd = x.grad\n",
    "        y_grad_scatch = scratch.backward(torch.ones_like(y_scratch))\n",
    "    \n",
    "    print(\"max error forward pass: \", torch.max(torch.abs(y_gd - y_scratch)))\n",
    "    print(\"max error backward pass: \", torch.max(torch.abs(y_grad_gd - y_grad_scatch)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max error forward pass:  tensor(0., grad_fn=<MaxBackward1>)\n",
      "max error backward pass:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        self.zero_mask = None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.zero_mask = x > 0\n",
    "        return x * self.zero_mask\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        result = self.zero_mask * gradwrtoutput\n",
    "        return result\n",
    "\n",
    "compare_fcts(nn.ReLU(), ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max error forward pass:  tensor(0., grad_fn=<MaxBackward1>)\n",
      "max error backward pass:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "class LeakyRelu(Module):\n",
    "    def __init__(self, slope):\n",
    "        self.slope = slope\n",
    "        self.zero_mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        zero_mask = x > 0\n",
    "        self.zero_mask = zero_mask.float()\n",
    "        result = zero_mask * (1 - self.slope)\n",
    "        result += self.slope\n",
    "        return x * result\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        result = self.zero_mask * (1 - self.slope)\n",
    "        result += self.slope \n",
    "        return result\n",
    "    \n",
    "\n",
    "compare_fcts(nn.LeakyReLU(), LeakyRelu(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max error forward pass:  tensor(1.1921e-07, grad_fn=<MaxBackward1>)\n",
      "max error backward pass:  tensor(5.9605e-08)\n"
     ]
    }
   ],
   "source": [
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        self.__e = torch.e\n",
    "        self.forward_sigm = None\n",
    "        self.mode = \"train\"\n",
    "\n",
    "    def __sig(self, x):\n",
    "        return 1 / (1 + self.__e ** -x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sig = self.__sig(x)\n",
    "        if self.mode == \"train\":\n",
    "            self.forward_sigm = sig\n",
    "        return sig\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        result = gradwrtoutput * self.forward_sigm * (1 - self.forward_sigm)\n",
    "        return result\n",
    "    \n",
    "\n",
    "compare_fcts(nn.Sigmoid(), Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (144) must match the size of tensor b (5) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m conv2_gd\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m conv2_scratch\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mclone()\n\u001b[1;32m    115\u001b[0m conv2_gd\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m conv2_scratch\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mclone()\n\u001b[0;32m--> 116\u001b[0m compare_fcts(conv2_gd, conv2_scratch)\n",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mcompare_fcts\u001b[0;34m(gd, scratch)\u001b[0m\n\u001b[1;32m      6\u001b[0m     y_gd\u001b[39m.\u001b[39mbackward(torch\u001b[39m.\u001b[39mones_like(y_gd))\n\u001b[1;32m      7\u001b[0m     y_grad_gd \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mgrad\n\u001b[0;32m----> 8\u001b[0m     y_grad_scatch \u001b[39m=\u001b[39m scratch\u001b[39m.\u001b[39;49mbackward(torch\u001b[39m.\u001b[39;49mones_like(y_scratch))\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmax error forward pass: \u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mmax(torch\u001b[39m.\u001b[39mabs(y_gd \u001b[39m-\u001b[39m y_scratch)))\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmax error backward pass: \u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mmax(torch\u001b[39m.\u001b[39mabs(y_grad_gd \u001b[39m-\u001b[39m y_grad_scatch)))\n",
      "Cell \u001b[0;32mIn[28], line 74\u001b[0m, in \u001b[0;36mConv2d.backward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m     71\u001b[0m grad_output_unfolded \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39munfold(grad_output, kernel_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size, stride\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, padding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, dilation\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)\n\u001b[1;32m     73\u001b[0m \u001b[39m# compute gradient w.r.t. weights\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m grad_w \u001b[39m=\u001b[39m grad_output_unfolded\u001b[39m.\u001b[39;49mtranspose(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mmatmul(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_unfolded\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m))\n\u001b[1;32m     75\u001b[0m grad_w \u001b[39m=\u001b[39m grad_w\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mview_as(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n\u001b[1;32m     77\u001b[0m \u001b[39m# compute gradient w.r.t. biases\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (144) must match the size of tensor b (5) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "class Conv2d(Module):\n",
    "    def __init__(self, channels_in=1, channels_out=1, kernel_size=(3, 3), stride=1, padding=0, dilation=1,\n",
    "                 bias=True):\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "        self.is_bias = bias\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.device = \"cpu\"\n",
    "        self.mode = \"train\"\n",
    "        self.input_unfolded = None\n",
    "        self.input_shape = None\n",
    "\n",
    "        self._initialize_parameters()\n",
    "    \n",
    "    def _initialize_parameters(self):\n",
    "        xavier_bound = (2 / (self.channels_out + self.channels_in) ** 0.5)\n",
    "        self.weight = torch.empty((self.channels_out, self.channels_in, *self.kernel_size)).uniform_(-xavier_bound, xavier_bound).to(self.device)\n",
    "        self.bias = torch.empty(self.channels_out).uniform_(-xavier_bound, xavier_bound).to(self.device) if self.is_bias else None\n",
    "        \n",
    "        self.dB = self.bias.clone() if self.is_bias else None\n",
    "        self.dW = self.weight.clone()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = input.to(self.device)\n",
    "        N, _, _, _ = input.shape\n",
    "        self.input_unfolded = torch.nn.functional.unfold(input, kernel_size=self.kernel_size, padding=self.padding,\n",
    "                                                         stride=self.stride, dilation=self.dilation).to(self.device)\n",
    "        wxb = self.weight.view(self.channels_out, -1) @ self.input_unfolded + self.bias.view(1, -1, 1)\n",
    "        result = wxb.view(N, self.channels_out,\n",
    "                          ((input.shape[2] - self.dilation * self.kernel_size[\n",
    "                              0] + 2 * self.padding) // self.stride + 1),\n",
    "                          ((input.shape[3] - self.dilation * self.kernel_size[\n",
    "                              1] + 2 * self.padding) // self.stride + 1))\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            self.input_shape = input.shape\n",
    "\n",
    "        return result\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        N, C, H, W = self.input_shape\n",
    "\n",
    "        # compute gradient w.r.t. weights\n",
    "        grad_reshaped = gradwrtoutput.permute(1, 2, 3, 0).reshape(self.channels_out, -1)\n",
    "        dW = grad_reshaped @ self.input_unfolded.permute(1, 2, 0).reshape(self.input_unfolded.shape[1], -1).T\n",
    "        self.dW.zero_()\n",
    "        self.dW += dW.reshape(self.weight.shape)\n",
    "\n",
    "        # compute gradient w.r.t. biases\n",
    "        self.dB.zero_()\n",
    "        self.dB += gradwrtoutput.sum(axis=(0, 2, 3)).view(-1)\n",
    "\n",
    "        # compute gradient w.r.t. inputs\n",
    "        weight_reshaped = self.weight.view(self.channels_out, -1)\n",
    "        dX_col = weight_reshaped.T @ grad_reshaped\n",
    "        out_h = (H - self.dilation * (self.kernel_size[0] - 1) + 2 * self.padding - 1) // self.stride + 1\n",
    "        out_w = (W - self.dilation * (self.kernel_size[1] - 1) + 2 * self.padding - 1) // self.stride + 1\n",
    "        dX_col_reshaped = dX_col.view(C * self.kernel_size[0] * self.kernel_size[1], out_w * out_h, N)\n",
    "        dX = torch.nn.functional.fold(dX_col_reshaped.permute(2, 0, 1),\n",
    "                                      self.input_shape[2:], kernel_size=self.kernel_size, padding=self.padding,\n",
    "                                      stride=self.stride, dilation=self.dilation)\n",
    "        \n",
    "        return dX\n",
    "\n",
    "\n",
    "    def _backward(self, grad_output):\n",
    "        N, C, H, W = self.input_shape\n",
    "        grad_output_unfolded = F.unfold(grad_output, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding, dilation=self.dilation)\n",
    "\n",
    "        # compute gradient w.r.t. weights\n",
    "        grad_w = grad_output_unfolded.transpose(0, 1).matmul(self.input_unfolded.transpose(1, 2))\n",
    "        grad_w = grad_w.transpose(0, 1).view_as(self.weight)\n",
    "\n",
    "        # compute gradient w.r.t. biases\n",
    "        grad_b = grad_output.sum(dim=(0, 2, 3))\n",
    "        if self.bias is not None:\n",
    "            grad_b = grad_b.view_as(self.bias)\n",
    "\n",
    "        # compute gradient w.r.t. inputs\n",
    "        grad_output_reshaped = grad_output_unfolded.transpose(0, 1)\n",
    "        grad_output_reshaped = grad_output_reshaped.view(C * self.kernel_size[0] * self.kernel_size[1], -1)\n",
    "        grad_input_unfolded = self.weight.view(self.weight.size(0), -1).matmul(grad_output_reshaped)\n",
    "        grad_input_unfolded = grad_input_unfolded.view(-1, self.input_unfolded.size(1), self.input_unfolded.size(2))\n",
    "        grad_input = F.fold(grad_input_unfolded, output_size=self.input_shape[2:], kernel_size=self.kernel_size, stride=self.stride, padding=self.padding, dilation=self.dilation)\n",
    "\n",
    "        return grad_input, grad_w, grad_b\n",
    "\n",
    "    \n",
    "    def get_params(self):\n",
    "        return [self.weight, self.bias, self.is_bias]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        self.weight = params[0].to(self.device)\n",
    "        self.bias = params[1].to(self.device)\n",
    "        self.is_bias = params[2]\n",
    "\n",
    "    def cuda(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.weight = self.weight.to(self.device)\n",
    "        self.bias = self.bias.to(self.device)\n",
    "        self.dW = self.dW.to(self.device)\n",
    "        self.dB = self.dB.to(self.device)\n",
    "        self.prev_update_conv = self.prev_update_conv.to(self.device)\n",
    "        self.prev_update_bias = self.prev_update_bias.to(self.device)\n",
    "\n",
    "    def params(self):\n",
    "        return [[self.weight, self.dW], [self.bias, self.dB]]\n",
    "\n",
    "conv2_gd = Conv2d(3, 16, 3, 1, 1)\n",
    "conv2_scratch = Conv2d(3, 16, 3, 1, 1)\n",
    "conv2_gd.weight = conv2_scratch.weight.clone()\n",
    "conv2_gd.bias = conv2_scratch.bias.clone()\n",
    "compare_fcts(conv2_gd, conv2_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.7 ms ± 3.42 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a = torch.randn((500,500,3,3))\n",
    "for i in range(1000):\n",
    "    a.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 ms ± 3.59 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a = torch.randn((500,500,3,3))\n",
    "for i in range(1000):\n",
    "    a *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max error forward pass:  tensor(0., grad_fn=<MaxBackward1>)\n",
      "max error backward pass:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "class MaxPool2d(Module):\n",
    "    def __init__(self):\n",
    "        self.index = None\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, c, W, H = x.shape\n",
    "        xmax = x.view(n, c, W // 2, 2, H // 2, 2).max(5).values.max(3)\n",
    "        xmax_max = xmax.values.repeat_interleave(2, axis=2).repeat_interleave(2, axis=3)\n",
    "        self.mask = xmax_max == x\n",
    "        return xmax.values\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        result = self.mask * gradwrtoutput.repeat_interleave(2, axis=2).repeat_interleave(2, axis=3)\n",
    "        return result    \n",
    "\n",
    "\n",
    "compare_fcts(nn.MaxPool2d(2), MaxPool2d())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max error forward pass:  tensor(0., grad_fn=<MaxBackward1>)\n",
      "max error backward pass:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class UpSampling2D(Module):\n",
    "    \"\"\" Nearest neighbor up sampling of the input. Repeats the rows and columns of the data by size[0] and size[1] respectively. Parameters: ----------- size: tuple (size_y, size_x) - The number of times each axis will be repeated. \"\"\"\n",
    "\n",
    "    def __init__(self, scale_factor=2):\n",
    "        self.scale_factor = scale_factor\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.repeat_interleave(self.scale_factor, dim=2).repeat_interleave(self.scale_factor, dim=3)\n",
    "        \n",
    "    def backward(self, dy):\n",
    "        N, C, H, W = dy.shape\n",
    "        dx = torch.zeros((N, C, H//self.scale_factor, W//self.scale_factor)).to(self.device)\n",
    "        for i in range(H//self.scale_factor):\n",
    "            for j in range(W//self.scale_factor):\n",
    "                dx[:, :, i, j] = dy[:, :, i*self.scale_factor:(i+1)*self.scale_factor, j*self.scale_factor:(j+1)*self.scale_factor].sum(dim=(-1, -2))\n",
    "        return dx\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "compare_fcts(nn.Upsample(scale_factor=2, mode='nearest'), UpSampling2D(scale_factor=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, *functions):\n",
    "        self.functions = functions\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for func in self.functions:\n",
    "            x = func.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        for func in reversed(self.functions):\n",
    "            grad = func.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def get_params(self):\n",
    "        return [func.get_params() for func in self.functions]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        for i, param in enumerate(params):\n",
    "            self.functions[i].set_params(param)\n",
    "\n",
    "    def cuda(self):\n",
    "        for func in self.functions:\n",
    "            func.cuda()\n",
    "\n",
    "    def parameters(self):\n",
    "        return [func.params() for func in self.functions]\n",
    "\n",
    "    def eval(self):\n",
    "        for func in self.functions:\n",
    "            func.eval()\n",
    "\n",
    "    def train(self):\n",
    "        for func in self.functions:\n",
    "            func.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max error forward pass:  tensor(0., grad_fn=<MaxBackward1>)\n",
      "max error backward pass:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "seq_test_scratch = Sequential(UpSampling2D(scale_factor=2), UpSampling2D(scale_factor=2), ReLU())\n",
    "seq_test_gd = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Upsample(scale_factor=2, mode='nearest'), nn.ReLU())\n",
    "\n",
    "compare_fcts(seq_test_gd, seq_test_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max error forward pass:  tensor(0., grad_fn=<MaxBackward1>)\n",
      "max error backward pass:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# skip connection block\n",
    "\n",
    "class SkipConnection(Module):\n",
    "\n",
    "    def __init__(self, module):\n",
    "        self.module = module\n",
    "        self.size_in = None\n",
    "        self.size_out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.size_in = x.shape\n",
    "        skip_connection = x\n",
    "        x = self.module.forward(x)\n",
    "        x =  torch.cat((x, skip_connection), dim=1)\n",
    "        self.size_out = x.shape\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dz):\n",
    "        N, C, H, W = dz.shape\n",
    "        dy = dz[:, :C - self.size_in[1], :, :]\n",
    "        dx = dz[:, C - self.size_in[1]:, :, :]\n",
    "        dy = self.module.backward(dy)\n",
    "        dx += dy\n",
    "        return dx\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.module.get_params()\n",
    "\n",
    "    def set_params(self, params):\n",
    "        self.module.set_params(params)\n",
    "\n",
    "    def cuda(self):\n",
    "        self.module.cuda()\n",
    "\n",
    "    def params(self):\n",
    "        return self.module.params()\n",
    "\n",
    "    def eval(self):\n",
    "        self.module.eval()\n",
    "\n",
    "    def train(self):\n",
    "        self.module.train()\n",
    "\n",
    "class SkipConnection_gd(nn.Module):\n",
    "    \n",
    "        def __init__(self, module):\n",
    "            super(SkipConnection_gd, self).__init__()\n",
    "            self.module = module\n",
    "    \n",
    "        def forward(self, x):\n",
    "            return torch.cat((self.module.forward(x), x), dim=1)\n",
    "\n",
    "conv2_gd = Conv2d(3, 16, 3, 1, 1)\n",
    "conv2_scratch = Conv2d(3, 16, 3, 1, 1)\n",
    "conv2_gd.weight = conv2_scratch.weight.clone()\n",
    "conv2_gd.bias = conv2_scratch.bias.clone()\n",
    "\n",
    "skip_test_scratch = SkipConnection(conv2_scratch)\n",
    "skip_test_gd = SkipConnection_gd(conv2_gd)\n",
    "\n",
    "compare_fcts(skip_test_gd, skip_test_scratch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max error forward pass:  tensor(0., grad_fn=<MaxBackward1>)\n",
      "max error backward pass:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "gd = SkipConnection_gd(conv2_gd)\n",
    "scratch = SkipConnection(conv2_scratch)\n",
    "\n",
    "x = torch.rand(1, 3, 25, 25, requires_grad=True)\n",
    "y_gd = gd(x)\n",
    "with torch.no_grad():\n",
    "    grad = torch.rand_like(y_gd)\n",
    "    y_scratch = scratch(x)\n",
    "    y_gd.backward(grad)\n",
    "    y_grad_gd = x.grad\n",
    "    y_grad_scatch = scratch.backward(grad)\n",
    "\n",
    "print(\"max error forward pass: \", torch.max(torch.abs(y_gd - y_scratch)))\n",
    "print(\"max error backward pass: \", torch.max(torch.abs(y_grad_gd - y_grad_scatch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        self.grad = None\n",
    "\n",
    "    def __call__(self, model_output, ground_truth):\n",
    "        return self.forward(model_output, ground_truth)\n",
    "\n",
    "    def forward(self, model_output, ground_truth):\n",
    "        self.grad = 2 * (model_output - ground_truth) / model_output.numel()\n",
    "        return ((ground_truth - model_output) ** 2).mean()\n",
    "\n",
    "    def backward(self):\n",
    "        return self.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            for elem in param:\n",
    "                elem[0] -= self.lr * elem[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise2Noise network\n",
    "\n",
    "def convblock(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, activation=LeakyRelu(0.1)):\n",
    "    return Sequential(\n",
    "        Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias),\n",
    "        activation\n",
    "    )\n",
    "\n",
    "\n",
    "class Noise2Noise(Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder01 = Sequential(\n",
    "            convblock(in_channels=3, out_channels=48, kernel_size=3, stride=1, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            convblock(in_channels=48, out_channels=48, kernel_size=3, stride=2, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            MaxPool2d()\n",
    "        )\n",
    "\n",
    "        self.encoder2 = Sequential(\n",
    "            convblock(in_channels=48, out_channels=48, kernel_size=3, stride=1, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            MaxPool2d()\n",
    "        )\n",
    "\n",
    "        self.encoder3 = Sequential(\n",
    "            convblock(in_channels=48, out_channels=48, kernel_size=3, stride=1, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            MaxPool2d()\n",
    "        )\n",
    "\n",
    "        self.encoder4 = Sequential(\n",
    "            convblock(in_channels=48, out_channels=48, kernel_size=3, stride=1, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            MaxPool2d()\n",
    "        )\n",
    "\n",
    "        self.encoder56 = Sequential(\n",
    "            convblock(in_channels=48, out_channels=48, kernel_size=3, stride=1, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            MaxPool2d(),\n",
    "            convblock(in_channels=48, out_channels=48, kernel_size=3, stride=1, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            UpSampling2D(scale_factor=2),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.decoder5ab = Sequential(\n",
    "            convblock(in_channels=96, out_channels=96, kernel_size=3, stride=1, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            UpSampling2D(scale_factor=2),\n",
    "        )\n",
    "\n",
    "        self.decoder4ab = Sequential(\n",
    "            convblock(in_channels=144, out_channels=96, kernel_size=3, stride=1, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            convblock(in_channels=96, out_channels=96, kernel_size=3, stride=1, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            UpSampling2D(scale_factor=2),\n",
    "        )\n",
    "\n",
    "        self.decoder3ab = Sequential(\n",
    "            convblock(in_channels=144, out_channels=96, kernel_size=3, stride=1, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            convblock(in_channels=96, out_channels=96, kernel_size=3, stride=1, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            UpSampling2D(scale_factor=2),\n",
    "        )\n",
    "\n",
    "        self.decoder2ab = Sequential(\n",
    "            convblock(in_channels=144, out_channels=96, kernel_size=3, stride=1, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            convblock(in_channels=96, out_channels=96, kernel_size=3, stride=1, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            UpSampling2D(scale_factor=2),\n",
    "        )\n",
    "\n",
    "        self.decoder1abc = Sequential(\n",
    "            convblock(in_channels=99, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            convblock(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1, bias=False, activation=LeakyRelu(0.1)),\n",
    "            Conv2d(channels_in=32, channels_out=3, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.skip1 = SkipConnection(self.encoder56)\n",
    "        self.skip2 = SkipConnection(Sequential(self.encoder4, self.skip1, self.decoder5ab))\n",
    "        self.skip3 = SkipConnection(Sequential(self.encoder3, self.skip2, self.decoder4ab))\n",
    "        self.skip4 = SkipConnection(Sequential(self.encoder2, self.skip3, self.decoder3ab))\n",
    "        self.skip5 = SkipConnection(Sequential(self.encoder01, self.skip4, self.decoder2ab))\n",
    "        self.net = Sequential(self.skip5, self.decoder1abc)\n",
    "\n",
    "        self.lr = 6.7\n",
    "\n",
    "        self.loss = MSELoss()\n",
    "        self.optimizer = Optimizer(self.net.params(), lr=self.lr)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "    def load_pretrained_model(self, model_path) -> None:\n",
    "        ## This loads the parameters saved in bestmodel .pth into the model\n",
    "        params = torch.load(model_path, map_location=self.device)\n",
    "        self.net.set_params(params)\n",
    "\n",
    "    def train(self, train_input, train_target, num_epochs) -> None:\n",
    "        train_input = (train_input.float() / 255).to(self.device)\n",
    "        train_target = (train_target.float() / 255).to(self.device)\n",
    "        if torch.cuda.is_available():\n",
    "            self.net.cuda()\n",
    "            self.optimizer = Optimizer(self.net.params(), lr=self.lr)\n",
    "        self.net.train()\n",
    "        n_data = len(train_input)\n",
    "        batch_size = 64\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'EPOCH: {epoch + 1}/{num_epochs}')\n",
    "            loss = []\n",
    "            for first in range(0, n_data, batch_size):\n",
    "                with torch.no_grad():\n",
    "                    last = first + batch_size\n",
    "                    x_batch, y_batch = train_input[first:last], train_target[first:last]\n",
    "\n",
    "                    results = self.net.forward(x_batch)\n",
    "                    loss_ = self.loss(results, y_batch)\n",
    "                    loss.append(loss_)\n",
    "                    self.net.backward(self.loss.backward())\n",
    "                    self.optimizer.step()\n",
    "\n",
    "            sum = 0\n",
    "            for val in loss:\n",
    "                sum += val\n",
    "            print(\n",
    "                f'{(first / batch_size)} / {(n_data // batch_size)} | loss: {(sum / (first / batch_size + 1))}')\n",
    "\n",
    "    def predict(self, test_input) -> torch.Tensor:\n",
    "        #: test_input : tensor of size (N1 , C, H, W) with values in range 0 -255 that has tobe denoised by the trained or the loaded network\n",
    "        #: returns a tensor of the size (N1 , C, H, W) with values in range 0 -255.\n",
    "        self.net.eval()\n",
    "\n",
    "        def normalization_cut(imgs):\n",
    "            imgs_shape = imgs.shape\n",
    "            imgs = imgs.flatten()\n",
    "            imgs[imgs < 0] = 0\n",
    "            imgs[imgs > 1] = 1\n",
    "            imgs = imgs.view(imgs_shape)\n",
    "            return imgs\n",
    "\n",
    "        return 255 * normalization_cut(self.net(test_input.float() / 255).to(self.device)).cpu()\n",
    "\n",
    "    def save(self, model_path):\n",
    "        pck_file = self.net.get_params()\n",
    "        torch.save(pck_file, model_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Noise2Noise' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m Noise2Noise()\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m----> 5\u001b[0m optimizer \u001b[39m=\u001b[39m Optimizer(model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Noise2Noise' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "model = Noise2Noise()\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "optimizer = Optimizer(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset1, dataset2):\n",
    "        self.datasets = torch.cat([dataset1[:, None], dataset2[:, None]], dim=1)\n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            # transforms.TrivialAugmentWide()\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.RandomVerticalFlip(0.5),\n",
    "            # transforms.RandomCrop(size=(32, 32)),\n",
    "            # transforms.ColorJitter(brightness=.5, hue=.3)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if torch.rand(1) > 0.5:\n",
    "            return self.transforms(self.datasets[i])\n",
    "        else:\n",
    "            return self.transforms(self.datasets[i, [1, 0]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datasets)\n",
    "\n",
    "def display_rgb(img):\n",
    "    plt.imshow(img.permute(1,2,0))\n",
    "    plt.show()\n",
    "\n",
    "def psnr_eval(model, noised, ground_truth, must_randomize=True):\n",
    "    def psnr(denoised, ground_truth):\n",
    "        mse = torch.mean((denoised.cpu() - ground_truth.cpu()) ** 2)\n",
    "        return -10 * torch.log10(mse + 10 ** -8)\n",
    "    clean_imgs = ground_truth.clone()\n",
    "    noised = noised.float()\n",
    "    ground_truth = ground_truth.float()\n",
    "\n",
    "    denoised = model.predict(noised) / 255\n",
    "\n",
    "    psnr_result = psnr(denoised, (ground_truth / 255)).item()\n",
    "    print(f'PSNR result: {psnr_result}dB')\n",
    "\n",
    "    nb_images = 3\n",
    "\n",
    "    f, axarr = plt.subplots(nb_images, 3)\n",
    "\n",
    "    if must_randomize:\n",
    "        nb_index = np.random.choice(len(noised), nb_images)\n",
    "    else:\n",
    "        nb_index = np.arange(nb_images)\n",
    "    axarr[0, 0].set_title(\"Noisy Images\")\n",
    "    axarr[0, 1].set_title(\"Denoised\")\n",
    "    axarr[0, 2].set_title(\"Ground Truth\")\n",
    "\n",
    "    for i, index in enumerate(nb_index):\n",
    "        axarr[i, 0].imshow(noised[index].permute(1,2,0).int())\n",
    "        axarr[i,0].get_yaxis().set_visible(False)\n",
    "        axarr[i,0].get_xaxis().set_visible(False)\n",
    "        axarr[i, 1].imshow(denoised[index].cpu().detach().permute(1,2,0))\n",
    "        axarr[i, 1].get_yaxis().set_visible(False)\n",
    "        axarr[i, 1].get_xaxis().set_visible(False)\n",
    "        axarr[i, 2].imshow(clean_imgs[index].permute(1,2,0))\n",
    "        axarr[i, 2].get_yaxis().set_visible(False)\n",
    "        axarr[i, 2].get_xaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'The model will be loaded on the {\"GPU\" if device == \"cuda\" else \"cpu\"}.')\n",
    "\n",
    "noisy_imgs_1, noisy_imgs_2 = torch.load('train_data.pkl')\n",
    "noisy_imgs, clean_imgs = torch.load('val_data.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_eval(model, noisy_imgs, clean_imgs, must_randomize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda117",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
